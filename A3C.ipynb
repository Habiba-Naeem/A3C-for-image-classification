{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import libraries"
      ],
      "metadata": {
        "id": "yNE0Ufnqy-fv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import multiprocessing\n",
        "multiprocessing.set_start_method('spawn')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "JHlZuk2BkO3n",
        "outputId": "48131d24-4eba-49a8-ae19-525efc32e75a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "context has already been set",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-2193564ef901>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_start_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spawn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/context.py\u001b[0m in \u001b[0;36mset_start_method\u001b[0;34m(self, method, force)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_start_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_actual_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'context has already been set'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_actual_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: context has already been set"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "import os.path\n",
        "import sys\n",
        "import numpy as np\n",
        "import math\n",
        "import sklearn.metrics as metrics\n",
        "from tqdm import tqdm\n",
        "from gym import Env, spaces\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "from tensorflow.python.keras import backend\n",
        "from tensorflow.keras.backend import clear_session\n",
        "\n",
        "\n",
        "import time\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "import csv\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.compat.v1 as tf1\n",
        "import tensorflow.keras as K\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "x0Tl6bi3uTiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import sys"
      ],
      "metadata": {
        "id": "H7y9nzCmnrE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "import random\n",
        "import time\n",
        "import copy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "1sTIQdsmne1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-Process Data and utility funcs\n"
      ],
      "metadata": {
        "id": "SKHrEBIijTE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_and_decode(dataset, batch_size, is_training, data_size,n_patients):\n",
        "    if is_training:\n",
        "        dataset = dataset.shuffle(buffer_size=data_size, reshuffle_each_iteration=True)\n",
        "        dataset = dataset.batch(batch_size, drop_remainder=False)\n",
        "        dataset = dataset.repeat(None)\n",
        "    else:\n",
        "        #dataset = dataset.prefetch(buffer_size=data_size, reshuffle_each_iteration=True // batch_size)\n",
        "        dataset = dataset.prefetch(buffer_size=data_size // batch_size)\n",
        "        dataset = dataset.batch(batch_size, drop_remainder=False)\n",
        "        dataset = dataset.repeat(None)\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "evvYftZetkSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def initialize_clinical_practice(clinical_cases_feat,clinical_cases_labels,dataset_size,n_classes,is_training,n_patients,set_distribution):\n",
        "\n",
        "    if is_training:\n",
        "        _, counts = np.unique(clinical_cases_labels, return_counts=True)\n",
        "\n",
        "        akiec = np.squeeze(np.take(clinical_cases_feat, np.where(clinical_cases_labels == 0), axis=0))\n",
        "        akiec_labels = np.squeeze(np.take(clinical_cases_labels, np.where(clinical_cases_labels == 0), axis=0))\n",
        "\n",
        "        bcc = np.squeeze(np.take(clinical_cases_feat, np.where(clinical_cases_labels == 1), axis=0))\n",
        "        bcc_labels = np.squeeze(np.take(clinical_cases_labels, np.where(clinical_cases_labels == 1), axis=0))\n",
        "\n",
        "        bkl = np.squeeze(np.take(clinical_cases_feat, np.where(clinical_cases_labels == 2), axis=0))\n",
        "        bkl_labels = np.squeeze(np.take(clinical_cases_labels, np.where(clinical_cases_labels == 2), axis=0))\n",
        "\n",
        "        df = np.squeeze(np.take(clinical_cases_feat, np.where(clinical_cases_labels == 3), axis=0))\n",
        "        df_labels = np.squeeze(np.take(clinical_cases_labels, np.where(clinical_cases_labels == 3), axis=0))\n",
        "\n",
        "        mel = np.squeeze(np.take(clinical_cases_feat, np.where(clinical_cases_labels == 4), axis=0))\n",
        "        mel_labels = np.squeeze(np.take(clinical_cases_labels, np.where(clinical_cases_labels == 4), axis=0))\n",
        "\n",
        "        nv = np.squeeze(np.take(clinical_cases_feat, np.where(clinical_cases_labels == 5), axis=0))\n",
        "        nv_labels = np.squeeze(np.take(clinical_cases_labels, np.where(clinical_cases_labels == 5), axis=0))\n",
        "\n",
        "        vasc = np.squeeze(np.take(clinical_cases_feat, np.where(clinical_cases_labels == 6), axis=0))\n",
        "        vasc_labels = np.squeeze(np.take(clinical_cases_labels, np.where(clinical_cases_labels == 6), axis=0))\n",
        "\n",
        "        akiec_set = tf.data.Dataset.from_tensor_slices((akiec, akiec_labels)).shuffle(buffer_size=counts[0],\n",
        "                                                                                      reshuffle_each_iteration=True).repeat()\n",
        "        bcc_set = tf.data.Dataset.from_tensor_slices((bcc, bcc_labels)).shuffle(buffer_size=counts[1],\n",
        "                                                                                reshuffle_each_iteration=True).repeat()\n",
        "        bkl_set = tf.data.Dataset.from_tensor_slices((bkl, bkl_labels)).shuffle(buffer_size=counts[2],\n",
        "                                                                                reshuffle_each_iteration=True).repeat()\n",
        "        df_set = tf.data.Dataset.from_tensor_slices((df, df_labels)).shuffle(buffer_size=counts[3],\n",
        "                                                                             reshuffle_each_iteration=True).repeat()\n",
        "        mel_set = tf.data.Dataset.from_tensor_slices((mel, mel_labels)).shuffle(buffer_size=counts[4],\n",
        "                                                                                reshuffle_each_iteration=True).repeat()\n",
        "        nv_set = tf.data.Dataset.from_tensor_slices((nv, nv_labels)).shuffle(buffer_size=counts[5],\n",
        "                                                                             reshuffle_each_iteration=True).repeat()\n",
        "        vasc_set = tf.data.Dataset.from_tensor_slices((vasc, vasc_labels)).shuffle(buffer_size=counts[6],\n",
        "                                                                                   reshuffle_each_iteration=True).repeat()\n",
        "\n",
        "        dataset_train = tf.data.Dataset.sample_from_datasets([akiec_set, bcc_set, bkl_set, df_set, mel_set, nv_set, vasc_set], weights=set_distribution)\n",
        "        dataset_train = dataset_train.batch(1)\n",
        "\n",
        "    else:\n",
        "        dataset_train = tf.data.Dataset.from_tensor_slices((clinical_cases_feat,clinical_cases_labels))\n",
        "\n",
        "        dataset_train = read_and_decode(dataset_train, 1, is_training, dataset_size,n_patients)\n",
        "\n",
        "    patients = iter(dataset_train)\n",
        "\n",
        "    return patients\n"
      ],
      "metadata": {
        "id": "VTFQP8-PtnW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_next_patient(patients):\n",
        "    patient_scores, patient_diagnostics = patients.get_next()\n",
        "    return np.squeeze(patient_scores), patient_diagnostics.numpy()[0]"
      ],
      "metadata": {
        "id": "ChWT-JKuttoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def torch_to_numpy(tensor):\n",
        "    return tensor.data.numpy()\n",
        "\n",
        "def numpy_to_torch(array):\n",
        "    return torch.tensor(array).float()"
      ],
      "metadata": {
        "id": "ov1_5TwbpJR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Shared Adam"
      ],
      "metadata": {
        "id": "40c61LaF5aX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch as T\n",
        "import torch.multiprocessing as mp\n",
        "class SharedAdam(T.optim.Adam):\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.99), eps=1e-8,\n",
        "            weight_decay=0, state_steps = [torch.tensor([0])]):\n",
        "        super(SharedAdam, self).__init__(params, lr=lr, betas=betas, eps=eps,\n",
        "                weight_decay=weight_decay, state_steps = [torch.tensor([0])])\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                state = self.state[p]\n",
        "                state['step'] = 0\n",
        "                state['exp_avg'] = T.zeros_like(p.data)\n",
        "                state['exp_avg_sq'] = T.zeros_like(p.data)\n",
        "\n",
        "                state['exp_avg'].share_memory_()\n",
        "                state['exp_avg_sq'].share_memory_()"
      ],
      "metadata": {
        "id": "MvyURjdc5eyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A2C Network"
      ],
      "metadata": {
        "id": "BSMf9IeojdXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Network(nn.Module):\n",
        "    def __init__(self, input_size, num_actions):\n",
        "        super(Network, self).__init__()\n",
        "\n",
        "        # Shared layers for processing combined input\n",
        "        self.fc1 = nn.Linear(input_size, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "\n",
        "        # Actor head\n",
        "        self.fc_actor = nn.Linear(64, num_actions)\n",
        "\n",
        "        # Critic head\n",
        "        self.fc_critic = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Shared layers\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "\n",
        "        # Actor head\n",
        "        policy_logits = self.fc_actor(x)\n",
        "\n",
        "        # Critic head\n",
        "        value_estimate = self.fc_critic(x)\n",
        "\n",
        "        return policy_logits, value_estimate"
      ],
      "metadata": {
        "id": "ObNqBKiVklo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A2C Environment"
      ],
      "metadata": {
        "id": "vFlkpzqsjg3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym import error, spaces, utils\n",
        "from gym.utils import seeding\n",
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class DermEnv(gym.Env):\n",
        "    def __init__(self, patients, vocab, n_patients):\n",
        "\n",
        "        # Actions we can take, either skin lesion classes or don't know\n",
        "        self.action_space = spaces.Discrete(len(vocab))\n",
        "        self.n_classes = len(vocab)\n",
        "\n",
        "        # Observation space - softmax + features after GAP\n",
        "        self.observation_space = spaces.Box(-1*math.inf*np.ones((self.n_classes,)),math.inf*np.ones((self.n_classes,)))\n",
        "\n",
        "        # Set shower length\n",
        "        self.number_of_patients = 1\n",
        "        self.n_patients = n_patients\n",
        "        self.patients = patients\n",
        "\n",
        "        # Initialize state\n",
        "        n_state, n_gt = get_next_patient(self.patients)\n",
        "\n",
        "        self.state = n_state\n",
        "        self.revised_state = self.state\n",
        "        self.gt = n_gt\n",
        "\n",
        "\n",
        "\n",
        "        print(\"State shape:\", self.state.shape)\n",
        "        print(\"Number actions:\", self.action_space) # 14 possible actions AI can play\n",
        "        print(\"Observation space:\", self.observation_space)\n",
        "\n",
        "\n",
        "    def step(self, patients, action):\n",
        "        ### UNKN_reward = -1\n",
        "        reward_table = np.array([[2,-2,-3,\t-3,\t-2,\t-3,\t-3,\t-1],\n",
        "                                 [-2,3,\t-4,\t-4,\t-2,\t-4,\t-4,\t-1],\n",
        "                                 [-2,-2, 1,\t-2,\t-3,\t-2,\t-2,\t-1],\n",
        "                                 [-2,-2,-2,\t1,-3,\t-2,\t-2,\t-1],\n",
        "                                 [-4,-3,-5,\t-5,\t5,-5,\t-5,\t-1],\n",
        "                                 [-2,-2,-2,\t-2,\t-3,\t1,\t-2,\t-1],\n",
        "                                 [-2,-2,-2,\t-2,\t-3,\t-2,\t1,\t-1],\n",
        "                                 ],np.float32)\n",
        "\n",
        "        self.revised_state = tf.one_hot(action, self.n_classes)\n",
        "\n",
        "        reward = reward_table[self.gt, action]\n",
        "\n",
        "        n_state, n_gt = get_next_patient(patients)\n",
        "\n",
        "        old_gt = self.gt\n",
        "\n",
        "        self.state = n_state\n",
        "        self.gt = n_gt\n",
        "\n",
        "        self.number_of_patients += 1\n",
        "\n",
        "        # Check if checking patients is done\n",
        "        if self.number_of_patients >=  self.n_patients:# or old_gt != action:\n",
        "            done = True\n",
        "        else:\n",
        "            done = False\n",
        "            #reward += 1000\n",
        "\n",
        "        return self.revised_state, self.state, reward, done, old_gt\n",
        "\n",
        "    def reset(self, patients_val, is_training):\n",
        "              # Reset clinical practice\n",
        "              if is_training:\n",
        "                n_state, n_gt = get_next_patient(self.patients)\n",
        "              else:\n",
        "                self.patients = patients_val\n",
        "                n_state, n_gt = get_next_patient(self.patients)\n",
        "\n",
        "              self.state = n_state\n",
        "              self.revised_state = self.state\n",
        "              self.gt = n_gt\n",
        "\n",
        "              # Reset new practice\n",
        "              self.number_of_patients = 1\n",
        "              return self.state,  self.patients\n"
      ],
      "metadata": {
        "id": "85D4japDLrD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A2C Actor and Critic"
      ],
      "metadata": {
        "id": "NtTCAtCrjkWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ActorCriticNNAgent():\n",
        "    '''\n",
        "    Neural-net agent that trains using the actor-critic algorithm. The critic\n",
        "    is a value function that returns expected discounted reward given the\n",
        "    state as input. We use advantage defined as\n",
        "\n",
        "        A = r + g * V(s') - V(s)\n",
        "\n",
        "    Notation:\n",
        "        A - advantage\n",
        "        V - value function\n",
        "        r - current reward\n",
        "        g - discount factor\n",
        "        s - current state\n",
        "        s' - next state\n",
        "    '''\n",
        "\n",
        "    def __init__(self, vocab, lr=1e-3, df=0.5, alpha=0.5):\n",
        "\n",
        "        # if trainable is changed to false, the model won't be updated\n",
        "        self.trainable = True\n",
        "        self.vocab = vocab\n",
        "        self.n_classes = len(self.vocab)\n",
        "\n",
        "        self.model = Network(519, self.n_classes)\n",
        "\n",
        "        if isinstance(self.model, torch.nn.Module):\n",
        "                    self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "\n",
        "        self.df = df # discount factor\n",
        "        self.alpha = alpha # multiply critic updates by this factor\n",
        "\n",
        "        # initialize replay history\n",
        "        self.replay = []\n",
        "    def act(self, state):\n",
        "        x = state\n",
        "        x = torch.tensor(x).float()\n",
        "        action_values, _ = self.model.forward(x)\n",
        "        action = torch.argmax(action_values).item()\n",
        "        if self.trainable:\n",
        "            # Convert state to NumPy array\n",
        "            state_np = state.numpy() if isinstance(state, torch.Tensor) else state\n",
        "            self.replay[-1]['states'].append(state_np)\n",
        "            self.replay[-1]['actions'].append(action)\n",
        "\n",
        "        return action\n",
        "\n",
        "\n",
        "    def new_episode(self):\n",
        "\n",
        "        # start a new episode in replay\n",
        "        self.replay.append({'states': [], 'actions': [], 'rewards': []})\n",
        "\n",
        "    def store_reward(self, r):\n",
        "        # insert 0s for actions that received no reward; end with reward r\n",
        "        episode = self.replay[-1]\n",
        "        episode['rewards'] +=  [r]\n",
        "\n",
        "    def _calculate_discounted_rewards(self):\n",
        "        # calculate and store discounted rewards per episode\n",
        "        for episode in self.replay:\n",
        "\n",
        "            R = episode['rewards']\n",
        "            R_disc = []\n",
        "            R_sum = 0\n",
        "            for r in R[::-1]:\n",
        "                R_sum = r + self.df * R_sum\n",
        "                R_disc.insert(0, R_sum)\n",
        "\n",
        "            episode['rewards_disc'] = R_disc\n",
        "\n",
        "    def update(self):\n",
        "\n",
        "        assert(self.trainable)\n",
        "\n",
        "        episode_losses = torch.tensor(0.0)\n",
        "        N = len(self.replay)\n",
        "        self._calculate_discounted_rewards()\n",
        "\n",
        "        for episode in self.replay:\n",
        "\n",
        "            O = episode['states']\n",
        "\n",
        "            A = episode['actions']\n",
        "            R = torch.tensor(episode['rewards']).float()\n",
        "            R_disc = torch.tensor(episode['rewards_disc']).float()\n",
        "            T = len(R_disc)\n",
        "            O_array = np.array(O)\n",
        "            x = torch.tensor(O_array).float()\n",
        "            Y1, Y2 = self.model.forward(x)\n",
        "            pi = Y1\n",
        "            Vs_curr = Y2.view(-1)\n",
        "\n",
        "            # log probabilities of selected actions\n",
        "            log_prob = torch.log(pi[range(T), A])\n",
        "\n",
        "            # advantage of selected actions over expected reward given state\n",
        "            Vs_next = torch.cat((Vs_curr[1:], torch.tensor([0.])))\n",
        "            adv = R + self.df * Vs_next - Vs_curr\n",
        "\n",
        "            # ignore gradients so the critic isn't affected by actor loss\n",
        "            adv = adv.detach()\n",
        "\n",
        "            # actor loss is -1 * advantage-weighted sum of log likelihood\n",
        "            # critic loss is the SE between values and discounted rewards\n",
        "            actor_loss = -torch.dot(log_prob, adv)\n",
        "            critic_loss = torch.sum((R_disc - Vs_curr) ** 2)\n",
        "            episode_losses += actor_loss + critic_loss * self.alpha\n",
        "\n",
        "        # backward pass\n",
        "        self.optimizer.zero_grad()\n",
        "        loss = episode_losses / N\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # reset the replay history\n",
        "        self.replay = []\n",
        "\n",
        "    def train_parallel(self, num_actors, iterations, episodes, patients):\n",
        "        self.patients = patients\n",
        "        self.envs = [DermEnv(patients=self.patients, vocab=self.vocab, n_patients=150) for _ in range(num_actors)]\n",
        "        self.actors = [ParallelActor(agent = self, env = self.envs[i], index = i, iterations = iterations, episodes = episodes, patients = self.patients) for i in range(num_actors)]\n",
        "\n",
        "        for actor in self.actors:\n",
        "            actor.start()\n",
        "        for actor in self.actors:\n",
        "            actor.join()\n",
        "\n",
        "\n",
        "    def test(self, iterations, patients):\n",
        "\n",
        "        self.trainable = False\n",
        "        self.patients = patients\n",
        "\n",
        "        error = np.array([])\n",
        "        true_label = np.array([])\n",
        "\n",
        "        rewards = []\n",
        "        rewards_history = []\n",
        "        self.episode_reward_history = []\n",
        "        self.episode_val_reward_history = []\n",
        "        self.validation_bacc_history = []\n",
        "        self.mel_history = []\n",
        "        self.unk_history = []\n",
        "\n",
        "        mel_count = 0\n",
        "        unk_count = 0\n",
        "        best_bacc = 0\n",
        "        best_reward = -1 * math.inf\n",
        "\n",
        "        for _ in range(iterations):\n",
        "\n",
        "            total_reward = 0\n",
        "            done = False\n",
        "            state, patients_val = self.env.reset(patients_val = self.patients, is_training =  False)\n",
        "\n",
        "            while not done:\n",
        "                true_label = np.append(true_label, self.env.gt)\n",
        "                action = self.act(state)\n",
        "                error = np.append(error, action)\n",
        "\n",
        "                if vocab[action] == 'mel':\n",
        "                    mel_count += 1\n",
        "                elif vocab[action] == 'unkn':\n",
        "                    unk_count += 1\n",
        "\n",
        "                _, state, reward, done, _ = self.env.step(patients_val, action)\n",
        "                total_reward += reward\n",
        "\n",
        "            rewards.append(total_reward)\n",
        "\n",
        "            print('The reward of the validation episode was ', total_reward)\n",
        "            print('The balanced accuracy was ', metrics.balanced_accuracy_score(true_label, error))\n",
        "            print('The accuracy was ', metrics.accuracy_score(true_label, error))\n",
        "\n",
        "            self.episode_val_reward_history.append(total_reward)\n",
        "            self.validation_bacc_history.append(metrics.balanced_accuracy_score(true_label, error))\n",
        "            self.mel_history.append(mel_count)\n",
        "            self.unk_history.append(unk_count)\n",
        "\n",
        "            if best_bacc < metrics.balanced_accuracy_score(true_label, error):\n",
        "                history_report_bacc = metrics.classification_report(true_label,error,digits=3)\n",
        "                history_cov_bacc = metrics.confusion_matrix(true_label,error)\n",
        "                best_bacc = metrics.balanced_accuracy_score(true_label, error)\n",
        "\n",
        "            if best_reward < total_reward:\n",
        "                history_cov_reward = metrics.confusion_matrix(true_label, error)\n",
        "                history_report_reward = metrics.classification_report(true_label,error,digits=3)\n",
        "                best_reward = total_reward\n",
        "\n",
        "        print('The scores for best validation BAcc are:')\n",
        "        print(history_report_bacc)\n",
        "        print(history_cov_bacc)\n",
        "        print('The best BAcc was ',best_bacc)\n",
        "\n",
        "        print('The scores for best validation Reward are:')\n",
        "        print(history_report_reward)\n",
        "        print(history_cov_reward)\n",
        "        print('The best reward was ', best_reward)\n",
        "\n",
        "    def plot_results(self):\n",
        "        if self.trainable:\n",
        "            plt.figure(1)\n",
        "            plt.plot(self.results)\n",
        "            plt.xlabel('Episode')\n",
        "            plt.ylabel('Reward')\n",
        "            plt.title('Training Progress')\n",
        "            plt.show()\n",
        "        else:\n",
        "            plt.figure(2)\n",
        "            plt.plot(self.episode_val_reward_history)\n",
        "            plt.xlabel('Episodes')\n",
        "            plt.ylabel('Reward Per Episode - Val')\n",
        "            plt.show()\n",
        "\n",
        "            plt.figure(3)\n",
        "            plt.plot(self.validation_bacc_history)\n",
        "            plt.xlabel('Episodes')\n",
        "            plt.ylabel('RL BAcc')\n",
        "            plt.show()\n",
        "\n",
        "            plt.figure(4)\n",
        "            plt.plot(self.mel_history)\n",
        "            plt.xlabel('Episodes')\n",
        "            plt.ylabel('Number Melanoma Decisions')\n",
        "            plt.show()"
      ],
      "metadata": {
        "id": "YbS76A_EgTRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Dataset"
      ],
      "metadata": {
        "id": "095q6d3QvLlc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### Import Datasets ####\n",
        "#tf1.enable_eager_execution()\n",
        "\n",
        "database = pd.read_csv('vectorDBNoSpace.csv')\n",
        "\n",
        "print(database)\n",
        "\n",
        "database.head()\n",
        "\n",
        "labels = np.asarray(database['dx'])\n",
        "\n",
        "print(labels)\n",
        "\n",
        "labels[labels == 'scc'] = 'akiec'\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "\n",
        "le.fit(labels)\n",
        "\n",
        "vocab = le.classes_\n",
        "\n",
        "n_words = len(vocab)\n",
        "\n",
        "features1 = np.load(\"nmed_rn34_ham10k_vectors.npy\")\n",
        "\n",
        "features2 = pd.read_csv(\"vectorDBNoSpace.csv\")\n",
        "\n",
        "features2.pop('dx')\n",
        "\n",
        "features2 = np.asarray(features2, dtype='float32')\n",
        "\n",
        "features = np.concatenate([features1,features2],axis=1)\n",
        "\n",
        "\n",
        "_, counts = np.unique(labels, return_counts=True)\n",
        "\n",
        "print(counts)\n",
        "\n",
        "counts = counts/np.sum(counts)\n",
        "\n",
        "\n",
        "labels_cat = le.transform(labels)\n",
        "\n",
        "print(labels_cat.dtype)\n",
        "\n",
        "train_feat, val_feat, train_labels, val_labels = train_test_split(features, labels_cat, test_size=0.2,\n",
        "                                                                      random_state=111,stratify = labels_cat)\n",
        "\n",
        "\n",
        "_, count_train = np.unique(val_labels, return_counts=True)\n",
        "\n",
        "print(count_train)\n",
        "\n",
        "patients = initialize_clinical_practice(train_feat,train_labels,train_labels.shape[0],True,n_words, 150,counts)\n",
        "\n",
        "patients_val = initialize_clinical_practice(val_feat,val_labels,val_labels.shape[0],False,n_words, 150,counts)\n",
        "\n"
      ],
      "metadata": {
        "id": "cCtxhc8OwORe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e575c0dd-7632-4f56-f079-56b4915257ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              akiec       bcc       bkl            df       mel        nv  \\\n",
            "0      1.239820e-04  0.001757  0.071755  2.613447e-02  0.004628  0.895575   \n",
            "1      1.564469e-03  0.000353  0.582653  1.401107e-02  0.000817  0.400582   \n",
            "2      7.310000e-07  0.000002  0.005028  5.290000e-07  0.002840  0.992129   \n",
            "3      3.012000e-03  0.027224  0.293466  3.917728e-02  0.062816  0.561710   \n",
            "4      1.044428e-03  0.007633  0.776036  3.400000e-05  0.197736  0.017514   \n",
            "...             ...       ...       ...           ...       ...       ...   \n",
            "10010  2.074046e-03  0.001095  0.002900  4.630000e-05  0.991719  0.000645   \n",
            "10011  9.210000e-06  0.000346  0.001581  4.070000e-05  0.308520  0.689383   \n",
            "10012  5.650000e-05  0.001307  0.977360  3.800000e-05  0.000746  0.020455   \n",
            "10013  3.850000e-06  0.000040  0.004911  2.747410e-04  0.214675  0.780093   \n",
            "10014  2.450000e-05  0.000041  0.013609  5.980000e-05  0.091868  0.894389   \n",
            "\n",
            "               vasc   dx  \n",
            "0      2.630000e-05   nv  \n",
            "1      1.920000e-05   nv  \n",
            "2      4.690000e-07   nv  \n",
            "3      1.259401e-02   nv  \n",
            "4      1.840000e-06  mel  \n",
            "...             ...  ...  \n",
            "10010  1.519908e-03  mel  \n",
            "10011  1.203480e-04  mel  \n",
            "10012  3.710000e-05  bkl  \n",
            "10013  2.210000e-06   nv  \n",
            "10014  8.780000e-06   nv  \n",
            "\n",
            "[10015 rows x 8 columns]\n",
            "['nv' 'nv' 'nv' ... 'bkl' 'nv' 'nv']\n",
            "[ 327  514 1099  115 1113 6705  142]\n",
            "int64\n",
            "[  65  103  220   23  223 1341   28]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ParallelActor(mp.Process):\n",
        "    def __init__(self, env, agent, index, iterations, episodes, patients):\n",
        "        super(ParallelActor, self).__init__()\n",
        "        self.env = env\n",
        "        self.agent = agent\n",
        "        self.index = index\n",
        "        self.iterations = iterations\n",
        "        self.episodes = episodes\n",
        "        self.patients = patients\n",
        "\n",
        "    def run(self):\n",
        "        self.agent.env = self.env  # Assign the environment to the agent\n",
        "        for iter in range(self.iterations):\n",
        "            rewards = []\n",
        "            for ep in range(self.episodes):\n",
        "                state, patients = self.agent.env.reset(patients_val=[], is_training=True)\n",
        "                self.agent.new_episode()\n",
        "                total_reward = 0\n",
        "                done = False\n",
        "                while not done:\n",
        "                    action = self.agent.act(state)\n",
        "                    _, state, reward, done, _ = self.agent.env.step(patients, action)\n",
        "                    self.agent.store_reward(reward)\n",
        "                    total_reward += reward\n",
        "                rewards.append(total_reward)\n",
        "                self.agent.update()\n",
        "            print(f\"Actor {self.index}: Mean total reward / episode: {np.mean(rewards)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SENML8Qf8P8",
        "outputId": "5ec7957f-a4a2-4f2b-8b4b-2a5540d19960"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ||"
      ],
      "metadata": {
        "id": "fgMwTvzCf5xD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start A2C"
      ],
      "metadata": {
        "id": "Ua0pCgEavO6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "agent = ActorCriticNNAgent(vocab = vocab)\n",
        "agent.train_parallel(5, 1,  1, patients = patients)\n",
        "agent.plot_results()\n",
        "#avg_reward = agent.test(iterations = 150, patients = patients_val)\n",
        "#agent.plot_results()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "H501bkJ7jvJp",
        "outputId": "bc5e82a0-8868-455d-b2a8-1100ae02ed75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State shape: (519,)\n",
            "Number actions: Discrete(7)\n",
            "Observation space: Box(-inf, inf, (7,), float32)\n",
            "State shape: (519,)\n",
            "Number actions: Discrete(7)\n",
            "Observation space: Box(-inf, inf, (7,), float32)\n",
            "State shape: (519,)\n",
            "Number actions: Discrete(7)\n",
            "Observation space: Box(-inf, inf, (7,), float32)\n",
            "State shape: (519,)\n",
            "Number actions: Discrete(7)\n",
            "Observation space: Box(-inf, inf, (7,), float32)\n",
            "State shape: (519,)\n",
            "Number actions: Discrete(7)\n",
            "Observation space: Box(-inf, inf, (7,), float32)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "Cannot convert a Tensor of dtype variant to a NumPy array.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-d903f84d537e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mActorCriticNNAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_parallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpatients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#avg_reward = agent.test(iterations = 150, patients = patients_val)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#agent.plot_results()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-e87486a2fbbc>\u001b[0m in \u001b[0;36mtrain_parallel\u001b[0;34m(self, num_actors, iterations, episodes, patients)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mactor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mactor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/process.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m                \u001b[0;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpopen_spawn_posix\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/popen_spawn_posix.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_launch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/popen_spawn_posix.py\u001b[0m in \u001b[0;36m_launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mset_spawning_popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/reduction.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(obj, file, protocol)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;34m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mForkingPickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__reduce__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__copy__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    360\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Cannot convert a Tensor of dtype variant to a NumPy array."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AIpFEYtOWJkv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}